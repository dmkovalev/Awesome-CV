% Encoding: UTF-8


@Article{BudzkoKalinichenkoStupnikovEtAl2014,
  Title                    = {Environment for Integration of Large Heterogeneous Data Collections},
  Author                   = {Budzko, V.I. and Kalinichenko, L.A. and Stupnikov, S.A. and Vovchenko, A.E. and Briukhov, D.O. and Kovalev, D.Yu.},
  Journal                  = {Highly Available Systems},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {3--19},
  Abstract                 = {New science and IT paradigm dominating during the last years is based on data exploration. Techniques for overcoming the diversity of data models (including semi-structured and unstructured data), metadata, and data semantics are required in the frame of this paradigm. Popularity of semi-structured NoSQL databases combined with Hadoop and MapReduce technologies aimed at parallel processing of large semi-structured data collections is steadily growing. Such recognition is explained by a multitude of actual and potential applications. This work concerns the area of development of data intensive systems. The aim of this work is analysis of approaches to the development of environment for integration of heterogeneous data collections. The environment should support both virtual and materialized integration of data collections represented in traditional and non-traditional data models. Virtual integration is provided by the subject mediation technology. The mediators form a layer between users (applications) and heterogeneous information resources. Materialized integration is proposed to be implemented using the Hadoop open source system for distributed data storage and processing. The Hadoop should be combined with a data warehouse system over Hadoop (Hive or IBM Big SQL systems can be used for that).

Basic features of the environment for integration of large heterogeneous collections are presented in the paper. Transformation of collections represented using non-traditional data models into the integrated data model is illustrated. The integrated representation is based on the warehouse data model. Brief overview of methods for information extraction from text, entity resolution and data fusion is provided. Techniques for programming of the entity resolution and data fusion methods using HIL high-level integration language are illustrated. An example of a problem to be solved using the proposed environment for integration of heterogeneous data collections is provided.},
  Keywords                 = {data integration, big data, software frameworks for distributed storage and processing of data, entity resolution, data fusion, subject mediators}
}

@Article{KalinichenkoKovalevKovalevaEtAl2015,
  Title                    = {Methods and Tools for Hypothesis-Driven Research Support: A Survey},
  Author                   = {Kalinichenko, L.A. and Kovalev, D.Yu. and Kovaleva, D.A. and Malkov, O.Yu.},
  Journal                  = {Informatics and Application},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {28--54},
  Volume                   = {9},
  Abstract                 = {Data intensive research (DIR) is being developed in frame of the new paradigmof research study known as the Fourth paradigm, emphasizing an increasing role of observational, experimental, and computer simulated data practically in all research domains. The principal goal of DIR is an extraction (inference) of knowledge from data. The intention of this work is to make an overview of the existing approaches, methods, and infrastructures of the data analysis in DIR accentuating the role of hypotheses in such process and efficient support of hypothesis formation, evaluation, and selection in course of the natural phenomena modeling and experiments carrying out.
An introduction into various concepts, methods, and tools intended for effective organization of hypothesis-driven experiments in DIR is presented.},
  DOI                      = {http://dx.doi.org/10.14357/19922264150104},
  Keywords                 = {data intensive research; Fourth paradigm; hypotheses; models; theories; hypothetico-deductivemethod; hypothesis testing; hypothesis lattice; Galaxy model; connectome analysis; automated hypothesis generation},
  Publisher                = {Institute of Informatics Problems, Russian Academy of Sciences},
  URL                      = {http://synthesis.ipi.ac.ru/synthesis/staff/dmkovalev/15ia-hypoth.pdf}
}

@InProceedings{KalinichenkoStupnikovVovchenkoEtAl2014,
  Title                    = {Multi-dialect Workflows},
  Author                   = {Kalinichenko, L. and Stupnikov, S. and Vovchenko, A. and Kovalev, D.},
  Booktitle                = {Advances in Databases and Information Systems},
  Year                     = {2014},
  Editor                   = {Manolopoulos, Y. and Trajcevski, G. and Kon-Popovska, M.},
  Organization             = {Springer},
  Pages                    = {352--365},
  Publisher                = {Springer International Publishing},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8716},
  Abstract                 = {The results presented in this paper contribute to the techniques for conceptual representation of data analysis algorithms as well as processes to specify data and behavior semantics in one paradigm. An investigation of a novel approach for applying a combination of semantically different platform independent rule-based languages (dialects) for interoperable conceptual specifications over various rule-based systems (RSs) relying on the rule-based program transformation technique recommended by the W3C Rule Interchange Format (RIF) is extended here. The approach is coupled also with the facilities for heterogeneous information resources mediation. This paper extends a previous research of the authors [1] in the direction of workflow modeling for definition of compositions of algorithmic modules in a process structure. A capability of the multi-dialect workflow support specifying the tasks in semantically different languages mostly suited to the task orientation is presented. A practical workflow use case, the interoperating tasks of which are specified in several rule-based languages (RIF-CASPD, RIF-BLD, RIF-PRD) is introduced. In addition, OWL 2 is used for the conceptual schema definition, RIF-PRD is used also for the workflow orchestration. The use case implementation infrastructure includes a production rule-based system (IBM ILOG), a logic rule-based system (DLV) and a mediation system.},
  DOI                      = {10.1007/978-3-319-10933-6_26},
  Keywords                 = {conceptual specification workflow RIF production rule languages database integration mediators PRD multi-dialect infrastructure}
}

@InCollection{KalinichenkoStupnikovVovchenkoEtAl2014a,
  Title                    = {Rule-based Multi-dialect Infrastructure for Conceptual Problem Solving over Heterogeneous Distributed Information Resources},
  Author                   = {Kalinichenko, L. and Stupnikov, S. and Vovchenko, A. and Kovalev, D.},
  Booktitle                = {New Trends in Databases and Information Systems},
  Publisher                = {Springer International Publishing},
  Year                     = {2013},
  Pages                    = {61--68},
  Series                   = {Advances in Intelligent Systems and Computing},
  Volume                   = {241},
  Abstract                 = {An approach for applying a combination of the semantically different rule-based languages for interoperable conceptual programming over various rule-based systems (RS) and relying on the logic program transformation technique recommended by the W3C Rule Interchange Format (RIF) is presented. Such approach is coherently combined with the heterogeneous data base integration applying semantic rule mediation. The basic functions of the infrastructure implementing the multi-dialect conceptual specifications by the interoperable RS and mediator programs are defined. The references to the detailed description of the infrastructure application for solving complex combinatorial problem are given. The research results show the usability of the approach and of the infrastructure for declarative, resource independent and re-usable data analysis in various application domains.},
  DOI                      = {10.1007/978-3-319-01863-8_7},
  Keywords                 = {conceptual specification RIF logic rule languages database integration mediators BLD CASPD multi-dialect infrastructure rule delegation}
}

@Article{SkvortsovVovchenkoKalinichenkoEtAl2014,
  Title                    = {A Metadata Model for Semantic Search of Rule-Based Workflow Implementations},
  Author                   = {Skvortsov, N.A. and Vovchenko, A.E. and Kalinichenko, L.A. and Kovalev, D.Yu. and Stupnikov, S.A.},
  Journal                  = {Systems and Means of Informatics},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {4--28},
  Volume                   = {24},
  Abstract                 = {Development of workflows and accumulation of open methods by scientific communities assumes their specification in open environment and their search to use them for problem solving. In this work, the Rule Interchange Format (RIF) dialects are used for workflow specification. Workflow elements are annotated with metadata to make their search in a subject domain possible.
The metadata model required for semantic search includes description of workflow skeleton structure, binding of workflow elements with concepts of the domain, quality and provenance requirements to data and methods. Metadata are defined in terms of corresponding ontologies. Search for workflow implementations in a collection of methods for construction of a workflow on the basis of existing relevant components is demonstrated by an example.},
  DOI                      = {10.14357/08696527140401},
  Keywords                 = {rule-based workflows; metadata; semantic search; ontology},
  Publisher                = {Institute of Informatics Problems, Russian Academy of Sciences}
}

@Article{VovchenkoKalinichenkoD.Yu.2014,
  Title                    = {Methods of Entity Resolution and Data Fusion In The ETL-process and their Implementation in the Hadoop Environment},
  Author                   = {Vovchenko, A.E. and Kalinichenko, L.A. and Kovalev D.Yu.},
  Journal                  = {Informatics and Applications},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {94 -- 109},
  Volume                   = {8},
  Abstract                 = {Entities extraction, their transformation and loading in the integrated repository are the main problem of data integration. These actions are part of the ETL-process (extract-transform-loading). An entity is a digital representation of a real world object (for example, information about a person). Entity resolution takes care of duplicate detection, deduplication, record linkage, object identification, reference matching, and other ETL- related tasks. After the entity resolution step, entities should be merged into the one reference entity (containing information from all related entities). Data fusion is the final step in the data integration process. The paper gives an overview of the entity resolution and data fusion methods. Also, the paper presents the techniques for programming the entity resolution and data fusion methods for implementing the ETL-process in the Hadoop environment. High-Level Integration Language (HIL), a declarative language that focuses on resolution and fusion of entities in the Hadoop-infrastructure, is used in this part of the paper.},
  DOI                      = {10.14357/19922264140412},
  Keywords                 = {data integration; ETL; entity resolution; data fusion; big data; Hadoop; Jaql; HIL}
}

